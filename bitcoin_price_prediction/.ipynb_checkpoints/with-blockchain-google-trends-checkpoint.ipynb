{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Imports](#Imports)\n",
    "2. [Setup of Initial Variables](#setup_init_variables)\n",
    "3. [Get Bitcoin Logo](#get_btc_logo)\n",
    "4. [Utility Functions for Data Preprocessing](#util_data_preprocess)\n",
    "5. [Download Data Sets](#download_datasets)\n",
    "    - [Extraction of Blockchain Data](#blockchain_data)\n",
    "    - [Extraction of Macroeconomic Data](#macroecon_data)\n",
    "    - [Extraction of Exchange Data](#exchange_data)<br><br>\n",
    "     - [Vizualize Exchange Data](#price_vizualization)\n",
    "     - [Preprocess Exchange Data](#preprocess_exch_data)\n",
    "     - [Plot Histogram of Daily Price Changes](#hist_daily_price_ch)<br><br>\n",
    "    - [Extraction of Global Currencies Exchange Data](#global_curr_exch_data)\n",
    "    - [Extraction of Sentiment Data](#sentiment_data)<br><br>\n",
    "7. [Merge of All Data Source](#merge_data_source)\n",
    "8. [Min-Max Scaling (a.k.a Normalization)](#normalization)\n",
    "9. [Statistic of Data Prior Training](#statistics)\n",
    "    - [Visualize the Whole Dataset](#viz_dataset)\n",
    "    - [Looking for Trends](#sesonality_differentiation)\n",
    "    - [Pearson Correlation](#pearson_corr)<br><br>\n",
    "10. [ML Pipeline](#ml_pipeline)\n",
    "   - [Split Data (Testing, Training Data Sets)](#split_data)\n",
    "   - [Set Window Length](#win_len_metrics)\n",
    "   - [Fill Training, Test Data](#fill_training_test)\n",
    "   - [Load Model](#load_model)\n",
    "   - [Train Model](#train_model)\n",
    "   - [Graph Predicted Values with Training Set](#graph_pred_training_set)\n",
    "   - [Graph Predicted Values with Test Data](#graph_pred_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptory import Cryptory\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request as urllib\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import quandl\n",
    "import time\n",
    "\n",
    "from_date=\"2013-04-28\"\n",
    "to_date=\"2018-10-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import plotly.plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# accounts = go.Scatter(\n",
    "#                 x=bch_accounts.Date,\n",
    "#                 y=bch_accounts['btc_Accounts'],\n",
    "#                 name = \"Accounts\",\n",
    "#                 line = dict(color = '#6D13C1'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# high = go.Scatter(\n",
    "#                 x=bitcoin_market_info.Date,\n",
    "#                 y=bitcoin_market_info['btc_High'],\n",
    "#                 name = \"High\",\n",
    "#                 line = dict(color = '#17BECF'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# low =go.Scatter(\n",
    "#                 x=bitcoin_market_info.Date,\n",
    "#                 y=bitcoin_market_info['btc_Low'],\n",
    "#                 name = \"Low\",\n",
    "#                 line = dict(color = '#7F7F7F'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# block_size =go.Scatter(\n",
    "#                 x=avg_block_size.Date,\n",
    "#                 y=avg_block_size['btc_avg_block_size'],\n",
    "#                 name = \"Avg. blockcsize\",\n",
    "#                 line = dict(color = '#D8B1B1'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# txs =go.Scatter(\n",
    "#                 x=txs_data.Date,\n",
    "#                 y=txs_data['btc_transactions'],\n",
    "#                 name = \"Nr of Transactions\",\n",
    "#                 line = dict(color = '#E125E8'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# miner_revenue =go.Scatter(\n",
    "#                 x=bchain_mirev_data.Date,\n",
    "#                 y=bchain_mirev_data['btc_mining_revenue'],\n",
    "#                 name = \"Miners Revenue\",\n",
    "#                 line = dict(color = '#3D0A3F'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# s_and_p =go.Scatter(\n",
    "#                 x=s_and_p_stock.Date,\n",
    "#                 y=s_and_p_stock['sp_close'],\n",
    "#                 name = \"S&P 500 Closing Price\",\n",
    "#                 line = dict(color = '#68340B'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "# dow_jones_stock =go.Scatter(\n",
    "#                 x=dow_jones_stock.Date,\n",
    "#                 y=dow_jones_stock['dj_close'],\n",
    "#                 name = \"Dow Jones Closing Price\",\n",
    "#                 line = dict(color = '#00E1FF'),\n",
    "#                 opacity = 0.8)\n",
    "\n",
    "\n",
    "# data = [low, high, block_size, txs, accounts, s_and_p, dow_jones_stock, miner_revenue]\n",
    "\n",
    "# layout = dict(\n",
    "#     title = \"Manually Set Date Range\",\n",
    "#     xaxis = dict(\n",
    "#         range = ['2014-01-01','2018-09-23'])\n",
    "# )\n",
    "\n",
    "# fig = dict(data=data, layout=layout)\n",
    "# py.iplot(fig, filename = \"Manually Set Range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Long Short Term Memory (LSTM)\n",
    "\n",
    "Like I said, if you're interested in the theory behind LSTMs, then I'll refer you to [this](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), [this](http://blog.echen.me/2017/05/30/exploring-lstms/) and [this](http://www.bioinf.jku.at/publications/older/2604.pdf). Luckily, we don't need to build the network from scratch (or even understand it), there exists packages that include standard implementations of various deep learning algorithms (e.g. [TensorFlow](https://www.tensorflow.org/get_started/get_started), [Keras](https://keras.io/#keras-the-python-deep-learning-library), [PyTorch](http://pytorch.org/), etc.). I'll opt for Keras, as I find it the most intuitive for non-experts. If you're not familiar with Keras, then check out my [previous tutorial](https://dashee87.github.io/data%20science/deep%20learning/python/another-keras-tutorial-for-neural-network-beginners/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>btc_high</th>\n",
       "      <th>btc_close</th>\n",
       "      <th>btc_volume</th>\n",
       "      <th>btc_market_cap</th>\n",
       "      <th>bch_avg_block_size</th>\n",
       "      <th>bch_transactions</th>\n",
       "      <th>bch_mining_revenue</th>\n",
       "      <th>bch_accounts</th>\n",
       "      <th>sp_close</th>\n",
       "      <th>dj_close</th>\n",
       "      <th>google_trends_bitcoin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1737</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>775.35</td>\n",
       "      <td>771.40</td>\n",
       "      <td>22489400</td>\n",
       "      <td>9203030016</td>\n",
       "      <td>0.102706</td>\n",
       "      <td>41476.0</td>\n",
       "      <td>3.741222e+06</td>\n",
       "      <td>962069.0</td>\n",
       "      <td>1848.359985</td>\n",
       "      <td>16576.660156</td>\n",
       "      <td>2.891802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1736</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>820.31</td>\n",
       "      <td>802.39</td>\n",
       "      <td>38489500</td>\n",
       "      <td>9428179968</td>\n",
       "      <td>0.102706</td>\n",
       "      <td>41476.0</td>\n",
       "      <td>2.775833e+06</td>\n",
       "      <td>976051.0</td>\n",
       "      <td>1831.979980</td>\n",
       "      <td>16441.349609</td>\n",
       "      <td>3.417584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1735</td>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>834.15</td>\n",
       "      <td>818.72</td>\n",
       "      <td>37810100</td>\n",
       "      <td>9786680320</td>\n",
       "      <td>0.199167</td>\n",
       "      <td>63821.0</td>\n",
       "      <td>3.296593e+06</td>\n",
       "      <td>976051.0</td>\n",
       "      <td>1831.369995</td>\n",
       "      <td>16469.990234</td>\n",
       "      <td>3.680475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1734</td>\n",
       "      <td>2014-01-04</td>\n",
       "      <td>859.51</td>\n",
       "      <td>859.51</td>\n",
       "      <td>38005000</td>\n",
       "      <td>10035600384</td>\n",
       "      <td>0.199167</td>\n",
       "      <td>63821.0</td>\n",
       "      <td>3.052675e+06</td>\n",
       "      <td>976051.0</td>\n",
       "      <td>1831.369995</td>\n",
       "      <td>16469.990234</td>\n",
       "      <td>3.286138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1733</td>\n",
       "      <td>2014-01-05</td>\n",
       "      <td>952.40</td>\n",
       "      <td>933.53</td>\n",
       "      <td>72898496</td>\n",
       "      <td>10465699840</td>\n",
       "      <td>0.167230</td>\n",
       "      <td>55761.0</td>\n",
       "      <td>4.043648e+06</td>\n",
       "      <td>993338.0</td>\n",
       "      <td>1831.369995</td>\n",
       "      <td>16469.990234</td>\n",
       "      <td>3.680475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1732</td>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>1017.12</td>\n",
       "      <td>953.29</td>\n",
       "      <td>85565696</td>\n",
       "      <td>11410400256</td>\n",
       "      <td>0.167230</td>\n",
       "      <td>55761.0</td>\n",
       "      <td>4.160971e+06</td>\n",
       "      <td>993338.0</td>\n",
       "      <td>1826.770020</td>\n",
       "      <td>16425.099609</td>\n",
       "      <td>4.469148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1731</td>\n",
       "      <td>2014-01-07</td>\n",
       "      <td>965.74</td>\n",
       "      <td>802.00</td>\n",
       "      <td>81311696</td>\n",
       "      <td>11571399680</td>\n",
       "      <td>0.174207</td>\n",
       "      <td>62126.0</td>\n",
       "      <td>4.068012e+06</td>\n",
       "      <td>993338.0</td>\n",
       "      <td>1837.880005</td>\n",
       "      <td>16530.939453</td>\n",
       "      <td>4.206257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1730</td>\n",
       "      <td>2014-01-08</td>\n",
       "      <td>870.68</td>\n",
       "      <td>842.72</td>\n",
       "      <td>74175600</td>\n",
       "      <td>9734800384</td>\n",
       "      <td>0.174207</td>\n",
       "      <td>62126.0</td>\n",
       "      <td>4.076107e+06</td>\n",
       "      <td>1014419.0</td>\n",
       "      <td>1837.489990</td>\n",
       "      <td>16462.740234</td>\n",
       "      <td>4.337703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1729</td>\n",
       "      <td>2014-01-09</td>\n",
       "      <td>864.36</td>\n",
       "      <td>846.86</td>\n",
       "      <td>59998600</td>\n",
       "      <td>10295100416</td>\n",
       "      <td>0.156845</td>\n",
       "      <td>58365.0</td>\n",
       "      <td>3.671204e+06</td>\n",
       "      <td>1014419.0</td>\n",
       "      <td>1838.130005</td>\n",
       "      <td>16444.759766</td>\n",
       "      <td>4.074812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1728</td>\n",
       "      <td>2014-01-10</td>\n",
       "      <td>871.19</td>\n",
       "      <td>868.48</td>\n",
       "      <td>31876800</td>\n",
       "      <td>10363299840</td>\n",
       "      <td>0.156845</td>\n",
       "      <td>58365.0</td>\n",
       "      <td>4.188603e+06</td>\n",
       "      <td>1014419.0</td>\n",
       "      <td>1842.369995</td>\n",
       "      <td>16437.050781</td>\n",
       "      <td>4.074812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1727</td>\n",
       "      <td>2014-01-11</td>\n",
       "      <td>921.48</td>\n",
       "      <td>913.95</td>\n",
       "      <td>44754200</td>\n",
       "      <td>10619899904</td>\n",
       "      <td>0.142487</td>\n",
       "      <td>56464.0</td>\n",
       "      <td>4.626026e+06</td>\n",
       "      <td>1032586.0</td>\n",
       "      <td>1842.369995</td>\n",
       "      <td>16437.050781</td>\n",
       "      <td>3.680475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1726</td>\n",
       "      <td>2014-01-12</td>\n",
       "      <td>928.52</td>\n",
       "      <td>863.22</td>\n",
       "      <td>39623500</td>\n",
       "      <td>11264799744</td>\n",
       "      <td>0.142487</td>\n",
       "      <td>56464.0</td>\n",
       "      <td>4.051611e+06</td>\n",
       "      <td>1032586.0</td>\n",
       "      <td>1842.369995</td>\n",
       "      <td>16437.050781</td>\n",
       "      <td>3.549029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1725</td>\n",
       "      <td>2014-01-13</td>\n",
       "      <td>861.29</td>\n",
       "      <td>841.20</td>\n",
       "      <td>45580900</td>\n",
       "      <td>10541500416</td>\n",
       "      <td>0.135760</td>\n",
       "      <td>54756.0</td>\n",
       "      <td>4.109431e+06</td>\n",
       "      <td>1032586.0</td>\n",
       "      <td>1819.199951</td>\n",
       "      <td>16257.940430</td>\n",
       "      <td>3.680475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1724</td>\n",
       "      <td>2014-01-14</td>\n",
       "      <td>855.69</td>\n",
       "      <td>833.27</td>\n",
       "      <td>20829800</td>\n",
       "      <td>10336499712</td>\n",
       "      <td>0.135760</td>\n",
       "      <td>54756.0</td>\n",
       "      <td>3.169736e+06</td>\n",
       "      <td>1050364.0</td>\n",
       "      <td>1838.880005</td>\n",
       "      <td>16373.860352</td>\n",
       "      <td>3.623208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1723</td>\n",
       "      <td>2014-01-15</td>\n",
       "      <td>872.81</td>\n",
       "      <td>860.90</td>\n",
       "      <td>28107200</td>\n",
       "      <td>10216699904</td>\n",
       "      <td>0.178593</td>\n",
       "      <td>60392.0</td>\n",
       "      <td>3.640024e+06</td>\n",
       "      <td>1050364.0</td>\n",
       "      <td>1848.380005</td>\n",
       "      <td>16481.939453</td>\n",
       "      <td>3.729773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1722</td>\n",
       "      <td>2014-01-16</td>\n",
       "      <td>866.16</td>\n",
       "      <td>835.63</td>\n",
       "      <td>19149300</td>\n",
       "      <td>10553799680</td>\n",
       "      <td>0.178593</td>\n",
       "      <td>60392.0</td>\n",
       "      <td>4.165302e+06</td>\n",
       "      <td>1050364.0</td>\n",
       "      <td>1845.890015</td>\n",
       "      <td>16417.009766</td>\n",
       "      <td>4.262598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1721</td>\n",
       "      <td>2014-01-17</td>\n",
       "      <td>842.91</td>\n",
       "      <td>814.64</td>\n",
       "      <td>39031700</td>\n",
       "      <td>10241200128</td>\n",
       "      <td>0.195853</td>\n",
       "      <td>58397.0</td>\n",
       "      <td>3.044327e+06</td>\n",
       "      <td>1069475.0</td>\n",
       "      <td>1838.699951</td>\n",
       "      <td>16458.560547</td>\n",
       "      <td>4.369163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1720</td>\n",
       "      <td>2014-01-18</td>\n",
       "      <td>841.49</td>\n",
       "      <td>840.00</td>\n",
       "      <td>18052700</td>\n",
       "      <td>10018199552</td>\n",
       "      <td>0.195853</td>\n",
       "      <td>58397.0</td>\n",
       "      <td>3.275853e+06</td>\n",
       "      <td>1069475.0</td>\n",
       "      <td>1838.699951</td>\n",
       "      <td>16458.560547</td>\n",
       "      <td>3.196948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1719</td>\n",
       "      <td>2014-01-19</td>\n",
       "      <td>870.96</td>\n",
       "      <td>870.96</td>\n",
       "      <td>24365700</td>\n",
       "      <td>10311499776</td>\n",
       "      <td>0.138307</td>\n",
       "      <td>49224.0</td>\n",
       "      <td>3.841796e+06</td>\n",
       "      <td>1069475.0</td>\n",
       "      <td>1838.699951</td>\n",
       "      <td>16458.560547</td>\n",
       "      <td>3.196948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1718</td>\n",
       "      <td>2014-01-20</td>\n",
       "      <td>886.39</td>\n",
       "      <td>870.20</td>\n",
       "      <td>27652700</td>\n",
       "      <td>10704799744</td>\n",
       "      <td>0.138307</td>\n",
       "      <td>49224.0</td>\n",
       "      <td>3.815931e+06</td>\n",
       "      <td>1087787.0</td>\n",
       "      <td>1838.699951</td>\n",
       "      <td>16458.560547</td>\n",
       "      <td>3.836338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1717</td>\n",
       "      <td>2014-01-21</td>\n",
       "      <td>881.20</td>\n",
       "      <td>863.91</td>\n",
       "      <td>18997300</td>\n",
       "      <td>10687699968</td>\n",
       "      <td>0.214486</td>\n",
       "      <td>71521.0</td>\n",
       "      <td>3.550169e+06</td>\n",
       "      <td>1087787.0</td>\n",
       "      <td>1843.800049</td>\n",
       "      <td>16414.439453</td>\n",
       "      <td>4.262598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1716</td>\n",
       "      <td>2014-01-22</td>\n",
       "      <td>870.15</td>\n",
       "      <td>845.59</td>\n",
       "      <td>18453700</td>\n",
       "      <td>10661399552</td>\n",
       "      <td>0.214486</td>\n",
       "      <td>71521.0</td>\n",
       "      <td>3.872557e+06</td>\n",
       "      <td>1087787.0</td>\n",
       "      <td>1844.859985</td>\n",
       "      <td>16373.339844</td>\n",
       "      <td>4.475728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1715</td>\n",
       "      <td>2014-01-23</td>\n",
       "      <td>851.57</td>\n",
       "      <td>822.04</td>\n",
       "      <td>15607100</td>\n",
       "      <td>10398000128</td>\n",
       "      <td>0.184911</td>\n",
       "      <td>63527.0</td>\n",
       "      <td>3.630947e+06</td>\n",
       "      <td>1109187.0</td>\n",
       "      <td>1828.459961</td>\n",
       "      <td>16197.349609</td>\n",
       "      <td>3.942903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1714</td>\n",
       "      <td>2014-01-24</td>\n",
       "      <td>822.43</td>\n",
       "      <td>797.07</td>\n",
       "      <td>34912000</td>\n",
       "      <td>10118400000</td>\n",
       "      <td>0.184911</td>\n",
       "      <td>63527.0</td>\n",
       "      <td>3.879216e+06</td>\n",
       "      <td>1109187.0</td>\n",
       "      <td>1790.290039</td>\n",
       "      <td>15879.110352</td>\n",
       "      <td>3.942903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1713</td>\n",
       "      <td>2014-01-25</td>\n",
       "      <td>861.45</td>\n",
       "      <td>853.61</td>\n",
       "      <td>24303900</td>\n",
       "      <td>9799919616</td>\n",
       "      <td>0.179263</td>\n",
       "      <td>55371.0</td>\n",
       "      <td>3.132922e+06</td>\n",
       "      <td>1109187.0</td>\n",
       "      <td>1790.290039</td>\n",
       "      <td>15879.110352</td>\n",
       "      <td>3.303513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1712</td>\n",
       "      <td>2014-01-26</td>\n",
       "      <td>897.02</td>\n",
       "      <td>885.28</td>\n",
       "      <td>32224300</td>\n",
       "      <td>10510399488</td>\n",
       "      <td>0.179263</td>\n",
       "      <td>55371.0</td>\n",
       "      <td>3.316174e+06</td>\n",
       "      <td>1130663.0</td>\n",
       "      <td>1790.290039</td>\n",
       "      <td>15879.110352</td>\n",
       "      <td>3.303513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1711</td>\n",
       "      <td>2014-01-27</td>\n",
       "      <td>893.00</td>\n",
       "      <td>771.39</td>\n",
       "      <td>49233600</td>\n",
       "      <td>10894399488</td>\n",
       "      <td>0.193695</td>\n",
       "      <td>60746.0</td>\n",
       "      <td>3.136749e+06</td>\n",
       "      <td>1130663.0</td>\n",
       "      <td>1781.560059</td>\n",
       "      <td>15837.879883</td>\n",
       "      <td>4.582293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1710</td>\n",
       "      <td>2014-01-28</td>\n",
       "      <td>832.50</td>\n",
       "      <td>812.51</td>\n",
       "      <td>44875500</td>\n",
       "      <td>9535949824</td>\n",
       "      <td>0.193695</td>\n",
       "      <td>60746.0</td>\n",
       "      <td>3.331233e+06</td>\n",
       "      <td>1130663.0</td>\n",
       "      <td>1792.500000</td>\n",
       "      <td>15928.559570</td>\n",
       "      <td>5.115117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1709</td>\n",
       "      <td>2014-01-29</td>\n",
       "      <td>836.87</td>\n",
       "      <td>826.00</td>\n",
       "      <td>17984400</td>\n",
       "      <td>9981849600</td>\n",
       "      <td>0.187368</td>\n",
       "      <td>61786.0</td>\n",
       "      <td>3.184667e+06</td>\n",
       "      <td>1151305.0</td>\n",
       "      <td>1774.199951</td>\n",
       "      <td>15738.790039</td>\n",
       "      <td>3.836338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1708</td>\n",
       "      <td>2014-01-30</td>\n",
       "      <td>830.50</td>\n",
       "      <td>819.03</td>\n",
       "      <td>29918200</td>\n",
       "      <td>10182899712</td>\n",
       "      <td>0.187368</td>\n",
       "      <td>61786.0</td>\n",
       "      <td>3.207872e+06</td>\n",
       "      <td>1151305.0</td>\n",
       "      <td>1794.189941</td>\n",
       "      <td>15848.610352</td>\n",
       "      <td>3.516643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>29</td>\n",
       "      <td>2018-09-05</td>\n",
       "      <td>7388.43</td>\n",
       "      <td>6792.83</td>\n",
       "      <td>5800460000</td>\n",
       "      <td>126983440721</td>\n",
       "      <td>1.008444</td>\n",
       "      <td>246477.0</td>\n",
       "      <td>1.171642e+07</td>\n",
       "      <td>28221242.0</td>\n",
       "      <td>2888.600098</td>\n",
       "      <td>25974.990234</td>\n",
       "      <td>7.699576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>28</td>\n",
       "      <td>2018-09-06</td>\n",
       "      <td>6755.14</td>\n",
       "      <td>6529.17</td>\n",
       "      <td>5523470000</td>\n",
       "      <td>116535447253</td>\n",
       "      <td>1.008444</td>\n",
       "      <td>246477.0</td>\n",
       "      <td>1.196295e+07</td>\n",
       "      <td>28247479.0</td>\n",
       "      <td>2878.050049</td>\n",
       "      <td>25995.869141</td>\n",
       "      <td>8.605408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>27</td>\n",
       "      <td>2018-09-07</td>\n",
       "      <td>6555.29</td>\n",
       "      <td>6467.07</td>\n",
       "      <td>4264680000</td>\n",
       "      <td>112645200318</td>\n",
       "      <td>0.889911</td>\n",
       "      <td>230790.0</td>\n",
       "      <td>1.226368e+07</td>\n",
       "      <td>28269613.0</td>\n",
       "      <td>2871.679932</td>\n",
       "      <td>25916.539062</td>\n",
       "      <td>6.642771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>26</td>\n",
       "      <td>2018-09-08</td>\n",
       "      <td>6534.25</td>\n",
       "      <td>6225.98</td>\n",
       "      <td>3835060000</td>\n",
       "      <td>111471362532</td>\n",
       "      <td>0.889911</td>\n",
       "      <td>230790.0</td>\n",
       "      <td>1.242508e+07</td>\n",
       "      <td>28293974.0</td>\n",
       "      <td>2871.679932</td>\n",
       "      <td>25916.539062</td>\n",
       "      <td>5.585967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>25</td>\n",
       "      <td>2018-09-09</td>\n",
       "      <td>6446.26</td>\n",
       "      <td>6300.86</td>\n",
       "      <td>3671890000</td>\n",
       "      <td>107397286192</td>\n",
       "      <td>0.616029</td>\n",
       "      <td>188912.0</td>\n",
       "      <td>1.205626e+07</td>\n",
       "      <td>28316824.0</td>\n",
       "      <td>2871.679932</td>\n",
       "      <td>25916.539062</td>\n",
       "      <td>5.434995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>24</td>\n",
       "      <td>2018-09-10</td>\n",
       "      <td>6374.98</td>\n",
       "      <td>6329.70</td>\n",
       "      <td>3714100000</td>\n",
       "      <td>108759350260</td>\n",
       "      <td>0.616029</td>\n",
       "      <td>188912.0</td>\n",
       "      <td>1.292408e+07</td>\n",
       "      <td>28342459.0</td>\n",
       "      <td>2877.129883</td>\n",
       "      <td>25857.070312</td>\n",
       "      <td>6.189855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>23</td>\n",
       "      <td>2018-09-11</td>\n",
       "      <td>6398.92</td>\n",
       "      <td>6321.20</td>\n",
       "      <td>3849910000</td>\n",
       "      <td>109293238395</td>\n",
       "      <td>0.941821</td>\n",
       "      <td>237571.0</td>\n",
       "      <td>1.075347e+07</td>\n",
       "      <td>28364675.0</td>\n",
       "      <td>2887.889893</td>\n",
       "      <td>25971.060547</td>\n",
       "      <td>5.887911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>22</td>\n",
       "      <td>2018-09-12</td>\n",
       "      <td>6363.87</td>\n",
       "      <td>6351.80</td>\n",
       "      <td>4064230000</td>\n",
       "      <td>109047547377</td>\n",
       "      <td>0.941821</td>\n",
       "      <td>237571.0</td>\n",
       "      <td>1.124381e+07</td>\n",
       "      <td>28389175.0</td>\n",
       "      <td>2888.919922</td>\n",
       "      <td>25998.919922</td>\n",
       "      <td>6.642771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>21</td>\n",
       "      <td>2018-09-13</td>\n",
       "      <td>6535.41</td>\n",
       "      <td>6517.31</td>\n",
       "      <td>4210910000</td>\n",
       "      <td>109703600477</td>\n",
       "      <td>0.819059</td>\n",
       "      <td>234209.0</td>\n",
       "      <td>1.332361e+07</td>\n",
       "      <td>28417954.0</td>\n",
       "      <td>2904.179932</td>\n",
       "      <td>26145.990234</td>\n",
       "      <td>6.340827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>20</td>\n",
       "      <td>2018-09-14</td>\n",
       "      <td>6596.10</td>\n",
       "      <td>6512.71</td>\n",
       "      <td>4076220000</td>\n",
       "      <td>112498115992</td>\n",
       "      <td>0.819059</td>\n",
       "      <td>234209.0</td>\n",
       "      <td>1.206376e+07</td>\n",
       "      <td>28439421.0</td>\n",
       "      <td>2904.979980</td>\n",
       "      <td>26154.669922</td>\n",
       "      <td>5.887911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>19</td>\n",
       "      <td>2018-09-15</td>\n",
       "      <td>6561.72</td>\n",
       "      <td>6543.20</td>\n",
       "      <td>3216300000</td>\n",
       "      <td>112405369809</td>\n",
       "      <td>0.789859</td>\n",
       "      <td>211071.0</td>\n",
       "      <td>1.086136e+07</td>\n",
       "      <td>28458932.0</td>\n",
       "      <td>2904.979980</td>\n",
       "      <td>26154.669922</td>\n",
       "      <td>5.133050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>18</td>\n",
       "      <td>2018-09-16</td>\n",
       "      <td>6544.33</td>\n",
       "      <td>6517.18</td>\n",
       "      <td>3273730000</td>\n",
       "      <td>112887643665</td>\n",
       "      <td>0.789859</td>\n",
       "      <td>211071.0</td>\n",
       "      <td>1.150999e+07</td>\n",
       "      <td>28487660.0</td>\n",
       "      <td>2904.979980</td>\n",
       "      <td>26154.669922</td>\n",
       "      <td>4.982078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>17</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>6540.21</td>\n",
       "      <td>6281.20</td>\n",
       "      <td>3910780000</td>\n",
       "      <td>112508446995</td>\n",
       "      <td>0.907648</td>\n",
       "      <td>240152.0</td>\n",
       "      <td>1.134482e+07</td>\n",
       "      <td>28510991.0</td>\n",
       "      <td>2888.800049</td>\n",
       "      <td>26062.119141</td>\n",
       "      <td>5.585967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>16</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>6384.18</td>\n",
       "      <td>6371.30</td>\n",
       "      <td>4180090000</td>\n",
       "      <td>108492112339</td>\n",
       "      <td>0.907648</td>\n",
       "      <td>240152.0</td>\n",
       "      <td>1.091275e+07</td>\n",
       "      <td>28533932.0</td>\n",
       "      <td>2904.310059</td>\n",
       "      <td>26246.960938</td>\n",
       "      <td>5.434995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>15</td>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>6448.46</td>\n",
       "      <td>6398.54</td>\n",
       "      <td>4431340000</td>\n",
       "      <td>110074137715</td>\n",
       "      <td>0.804437</td>\n",
       "      <td>251037.0</td>\n",
       "      <td>1.129624e+07</td>\n",
       "      <td>28568175.0</td>\n",
       "      <td>2907.949951</td>\n",
       "      <td>26405.759766</td>\n",
       "      <td>5.736939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>14</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>6529.26</td>\n",
       "      <td>6519.67</td>\n",
       "      <td>4348110000</td>\n",
       "      <td>110553109981</td>\n",
       "      <td>0.804437</td>\n",
       "      <td>251037.0</td>\n",
       "      <td>1.246316e+07</td>\n",
       "      <td>28597622.0</td>\n",
       "      <td>2930.750000</td>\n",
       "      <td>26656.980469</td>\n",
       "      <td>5.434995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>13</td>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>6794.33</td>\n",
       "      <td>6734.95</td>\n",
       "      <td>6531940000</td>\n",
       "      <td>112552829858</td>\n",
       "      <td>1.012302</td>\n",
       "      <td>273051.0</td>\n",
       "      <td>1.247295e+07</td>\n",
       "      <td>28627579.0</td>\n",
       "      <td>2929.669922</td>\n",
       "      <td>26743.500000</td>\n",
       "      <td>6.189855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>12</td>\n",
       "      <td>2018-09-22</td>\n",
       "      <td>6814.56</td>\n",
       "      <td>6721.98</td>\n",
       "      <td>4509660000</td>\n",
       "      <td>116386818047</td>\n",
       "      <td>1.012302</td>\n",
       "      <td>273051.0</td>\n",
       "      <td>1.217682e+07</td>\n",
       "      <td>28651311.0</td>\n",
       "      <td>2929.669922</td>\n",
       "      <td>26743.500000</td>\n",
       "      <td>5.284022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>11</td>\n",
       "      <td>2018-09-23</td>\n",
       "      <td>6766.15</td>\n",
       "      <td>6710.63</td>\n",
       "      <td>4197500000</td>\n",
       "      <td>116058736872</td>\n",
       "      <td>0.655633</td>\n",
       "      <td>200319.0</td>\n",
       "      <td>1.312446e+07</td>\n",
       "      <td>28682122.0</td>\n",
       "      <td>2929.669922</td>\n",
       "      <td>26743.500000</td>\n",
       "      <td>4.982078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>10</td>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>6713.56</td>\n",
       "      <td>6595.41</td>\n",
       "      <td>4177310000</td>\n",
       "      <td>115889159338</td>\n",
       "      <td>0.655633</td>\n",
       "      <td>200319.0</td>\n",
       "      <td>1.267923e+07</td>\n",
       "      <td>28701097.0</td>\n",
       "      <td>2919.370117</td>\n",
       "      <td>26562.050781</td>\n",
       "      <td>5.284022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>9</td>\n",
       "      <td>2018-09-25</td>\n",
       "      <td>6603.64</td>\n",
       "      <td>6446.47</td>\n",
       "      <td>4726180000</td>\n",
       "      <td>114153287747</td>\n",
       "      <td>0.768241</td>\n",
       "      <td>252113.0</td>\n",
       "      <td>1.238568e+07</td>\n",
       "      <td>28724040.0</td>\n",
       "      <td>2915.560059</td>\n",
       "      <td>26492.210938</td>\n",
       "      <td>5.736939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>8</td>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>6585.91</td>\n",
       "      <td>6495.00</td>\n",
       "      <td>4437300000</td>\n",
       "      <td>111559240252</td>\n",
       "      <td>0.768241</td>\n",
       "      <td>252113.0</td>\n",
       "      <td>1.367660e+07</td>\n",
       "      <td>28754595.0</td>\n",
       "      <td>2905.969971</td>\n",
       "      <td>26385.279297</td>\n",
       "      <td>5.434995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>7</td>\n",
       "      <td>2018-09-27</td>\n",
       "      <td>6712.10</td>\n",
       "      <td>6676.75</td>\n",
       "      <td>4606810000</td>\n",
       "      <td>112305283440</td>\n",
       "      <td>0.877031</td>\n",
       "      <td>248686.0</td>\n",
       "      <td>1.136949e+07</td>\n",
       "      <td>28787814.0</td>\n",
       "      <td>2914.000000</td>\n",
       "      <td>26439.929688</td>\n",
       "      <td>5.284022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>6</td>\n",
       "      <td>2018-09-28</td>\n",
       "      <td>6785.03</td>\n",
       "      <td>6644.13</td>\n",
       "      <td>5014430000</td>\n",
       "      <td>115489513453</td>\n",
       "      <td>0.877031</td>\n",
       "      <td>248686.0</td>\n",
       "      <td>1.183675e+07</td>\n",
       "      <td>28815399.0</td>\n",
       "      <td>2913.979980</td>\n",
       "      <td>26458.310547</td>\n",
       "      <td>5.434995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-09-29</td>\n",
       "      <td>6643.10</td>\n",
       "      <td>6601.96</td>\n",
       "      <td>4363690000</td>\n",
       "      <td>114885697090</td>\n",
       "      <td>0.693384</td>\n",
       "      <td>218339.0</td>\n",
       "      <td>1.304698e+07</td>\n",
       "      <td>28837177.0</td>\n",
       "      <td>2913.979980</td>\n",
       "      <td>26458.310547</td>\n",
       "      <td>4.529162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-30</td>\n",
       "      <td>6643.78</td>\n",
       "      <td>6625.56</td>\n",
       "      <td>4002280000</td>\n",
       "      <td>114234369232</td>\n",
       "      <td>0.693384</td>\n",
       "      <td>218339.0</td>\n",
       "      <td>1.257106e+07</td>\n",
       "      <td>28869795.0</td>\n",
       "      <td>2913.979980</td>\n",
       "      <td>26458.310547</td>\n",
       "      <td>4.529162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>6653.30</td>\n",
       "      <td>6589.62</td>\n",
       "      <td>4000970000</td>\n",
       "      <td>114509724600</td>\n",
       "      <td>0.857106</td>\n",
       "      <td>242820.0</td>\n",
       "      <td>1.353523e+07</td>\n",
       "      <td>28888424.0</td>\n",
       "      <td>2924.590088</td>\n",
       "      <td>26651.210938</td>\n",
       "      <td>4.831106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>6611.84</td>\n",
       "      <td>6556.10</td>\n",
       "      <td>3979260000</td>\n",
       "      <td>114062551875</td>\n",
       "      <td>0.857106</td>\n",
       "      <td>242820.0</td>\n",
       "      <td>1.316100e+07</td>\n",
       "      <td>28923372.0</td>\n",
       "      <td>2923.429932</td>\n",
       "      <td>26773.939453</td>\n",
       "      <td>4.982078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>6571.46</td>\n",
       "      <td>6502.59</td>\n",
       "      <td>3887310000</td>\n",
       "      <td>113392236466</td>\n",
       "      <td>0.838226</td>\n",
       "      <td>242303.0</td>\n",
       "      <td>1.140733e+07</td>\n",
       "      <td>28946409.0</td>\n",
       "      <td>2925.510010</td>\n",
       "      <td>26828.390625</td>\n",
       "      <td>5.133050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-10-04</td>\n",
       "      <td>6603.31</td>\n",
       "      <td>6576.69</td>\n",
       "      <td>3838410000</td>\n",
       "      <td>112435991226</td>\n",
       "      <td>0.838226</td>\n",
       "      <td>242303.0</td>\n",
       "      <td>1.186584e+07</td>\n",
       "      <td>28964820.0</td>\n",
       "      <td>2901.610107</td>\n",
       "      <td>26627.480469</td>\n",
       "      <td>4.982078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1738 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        date  btc_high  btc_close  btc_volume  btc_market_cap  \\\n",
       "0           1737  2014-01-01    775.35     771.40    22489400      9203030016   \n",
       "1           1736  2014-01-02    820.31     802.39    38489500      9428179968   \n",
       "2           1735  2014-01-03    834.15     818.72    37810100      9786680320   \n",
       "3           1734  2014-01-04    859.51     859.51    38005000     10035600384   \n",
       "4           1733  2014-01-05    952.40     933.53    72898496     10465699840   \n",
       "5           1732  2014-01-06   1017.12     953.29    85565696     11410400256   \n",
       "6           1731  2014-01-07    965.74     802.00    81311696     11571399680   \n",
       "7           1730  2014-01-08    870.68     842.72    74175600      9734800384   \n",
       "8           1729  2014-01-09    864.36     846.86    59998600     10295100416   \n",
       "9           1728  2014-01-10    871.19     868.48    31876800     10363299840   \n",
       "10          1727  2014-01-11    921.48     913.95    44754200     10619899904   \n",
       "11          1726  2014-01-12    928.52     863.22    39623500     11264799744   \n",
       "12          1725  2014-01-13    861.29     841.20    45580900     10541500416   \n",
       "13          1724  2014-01-14    855.69     833.27    20829800     10336499712   \n",
       "14          1723  2014-01-15    872.81     860.90    28107200     10216699904   \n",
       "15          1722  2014-01-16    866.16     835.63    19149300     10553799680   \n",
       "16          1721  2014-01-17    842.91     814.64    39031700     10241200128   \n",
       "17          1720  2014-01-18    841.49     840.00    18052700     10018199552   \n",
       "18          1719  2014-01-19    870.96     870.96    24365700     10311499776   \n",
       "19          1718  2014-01-20    886.39     870.20    27652700     10704799744   \n",
       "20          1717  2014-01-21    881.20     863.91    18997300     10687699968   \n",
       "21          1716  2014-01-22    870.15     845.59    18453700     10661399552   \n",
       "22          1715  2014-01-23    851.57     822.04    15607100     10398000128   \n",
       "23          1714  2014-01-24    822.43     797.07    34912000     10118400000   \n",
       "24          1713  2014-01-25    861.45     853.61    24303900      9799919616   \n",
       "25          1712  2014-01-26    897.02     885.28    32224300     10510399488   \n",
       "26          1711  2014-01-27    893.00     771.39    49233600     10894399488   \n",
       "27          1710  2014-01-28    832.50     812.51    44875500      9535949824   \n",
       "28          1709  2014-01-29    836.87     826.00    17984400      9981849600   \n",
       "29          1708  2014-01-30    830.50     819.03    29918200     10182899712   \n",
       "...          ...         ...       ...        ...         ...             ...   \n",
       "1708          29  2018-09-05   7388.43    6792.83  5800460000    126983440721   \n",
       "1709          28  2018-09-06   6755.14    6529.17  5523470000    116535447253   \n",
       "1710          27  2018-09-07   6555.29    6467.07  4264680000    112645200318   \n",
       "1711          26  2018-09-08   6534.25    6225.98  3835060000    111471362532   \n",
       "1712          25  2018-09-09   6446.26    6300.86  3671890000    107397286192   \n",
       "1713          24  2018-09-10   6374.98    6329.70  3714100000    108759350260   \n",
       "1714          23  2018-09-11   6398.92    6321.20  3849910000    109293238395   \n",
       "1715          22  2018-09-12   6363.87    6351.80  4064230000    109047547377   \n",
       "1716          21  2018-09-13   6535.41    6517.31  4210910000    109703600477   \n",
       "1717          20  2018-09-14   6596.10    6512.71  4076220000    112498115992   \n",
       "1718          19  2018-09-15   6561.72    6543.20  3216300000    112405369809   \n",
       "1719          18  2018-09-16   6544.33    6517.18  3273730000    112887643665   \n",
       "1720          17  2018-09-17   6540.21    6281.20  3910780000    112508446995   \n",
       "1721          16  2018-09-18   6384.18    6371.30  4180090000    108492112339   \n",
       "1722          15  2018-09-19   6448.46    6398.54  4431340000    110074137715   \n",
       "1723          14  2018-09-20   6529.26    6519.67  4348110000    110553109981   \n",
       "1724          13  2018-09-21   6794.33    6734.95  6531940000    112552829858   \n",
       "1725          12  2018-09-22   6814.56    6721.98  4509660000    116386818047   \n",
       "1726          11  2018-09-23   6766.15    6710.63  4197500000    116058736872   \n",
       "1727          10  2018-09-24   6713.56    6595.41  4177310000    115889159338   \n",
       "1728           9  2018-09-25   6603.64    6446.47  4726180000    114153287747   \n",
       "1729           8  2018-09-26   6585.91    6495.00  4437300000    111559240252   \n",
       "1730           7  2018-09-27   6712.10    6676.75  4606810000    112305283440   \n",
       "1731           6  2018-09-28   6785.03    6644.13  5014430000    115489513453   \n",
       "1732           5  2018-09-29   6643.10    6601.96  4363690000    114885697090   \n",
       "1733           4  2018-09-30   6643.78    6625.56  4002280000    114234369232   \n",
       "1734           3  2018-10-01   6653.30    6589.62  4000970000    114509724600   \n",
       "1735           2  2018-10-02   6611.84    6556.10  3979260000    114062551875   \n",
       "1736           1  2018-10-03   6571.46    6502.59  3887310000    113392236466   \n",
       "1737           0  2018-10-04   6603.31    6576.69  3838410000    112435991226   \n",
       "\n",
       "      bch_avg_block_size  bch_transactions  bch_mining_revenue  bch_accounts  \\\n",
       "0               0.102706           41476.0        3.741222e+06      962069.0   \n",
       "1               0.102706           41476.0        2.775833e+06      976051.0   \n",
       "2               0.199167           63821.0        3.296593e+06      976051.0   \n",
       "3               0.199167           63821.0        3.052675e+06      976051.0   \n",
       "4               0.167230           55761.0        4.043648e+06      993338.0   \n",
       "5               0.167230           55761.0        4.160971e+06      993338.0   \n",
       "6               0.174207           62126.0        4.068012e+06      993338.0   \n",
       "7               0.174207           62126.0        4.076107e+06     1014419.0   \n",
       "8               0.156845           58365.0        3.671204e+06     1014419.0   \n",
       "9               0.156845           58365.0        4.188603e+06     1014419.0   \n",
       "10              0.142487           56464.0        4.626026e+06     1032586.0   \n",
       "11              0.142487           56464.0        4.051611e+06     1032586.0   \n",
       "12              0.135760           54756.0        4.109431e+06     1032586.0   \n",
       "13              0.135760           54756.0        3.169736e+06     1050364.0   \n",
       "14              0.178593           60392.0        3.640024e+06     1050364.0   \n",
       "15              0.178593           60392.0        4.165302e+06     1050364.0   \n",
       "16              0.195853           58397.0        3.044327e+06     1069475.0   \n",
       "17              0.195853           58397.0        3.275853e+06     1069475.0   \n",
       "18              0.138307           49224.0        3.841796e+06     1069475.0   \n",
       "19              0.138307           49224.0        3.815931e+06     1087787.0   \n",
       "20              0.214486           71521.0        3.550169e+06     1087787.0   \n",
       "21              0.214486           71521.0        3.872557e+06     1087787.0   \n",
       "22              0.184911           63527.0        3.630947e+06     1109187.0   \n",
       "23              0.184911           63527.0        3.879216e+06     1109187.0   \n",
       "24              0.179263           55371.0        3.132922e+06     1109187.0   \n",
       "25              0.179263           55371.0        3.316174e+06     1130663.0   \n",
       "26              0.193695           60746.0        3.136749e+06     1130663.0   \n",
       "27              0.193695           60746.0        3.331233e+06     1130663.0   \n",
       "28              0.187368           61786.0        3.184667e+06     1151305.0   \n",
       "29              0.187368           61786.0        3.207872e+06     1151305.0   \n",
       "...                  ...               ...                 ...           ...   \n",
       "1708            1.008444          246477.0        1.171642e+07    28221242.0   \n",
       "1709            1.008444          246477.0        1.196295e+07    28247479.0   \n",
       "1710            0.889911          230790.0        1.226368e+07    28269613.0   \n",
       "1711            0.889911          230790.0        1.242508e+07    28293974.0   \n",
       "1712            0.616029          188912.0        1.205626e+07    28316824.0   \n",
       "1713            0.616029          188912.0        1.292408e+07    28342459.0   \n",
       "1714            0.941821          237571.0        1.075347e+07    28364675.0   \n",
       "1715            0.941821          237571.0        1.124381e+07    28389175.0   \n",
       "1716            0.819059          234209.0        1.332361e+07    28417954.0   \n",
       "1717            0.819059          234209.0        1.206376e+07    28439421.0   \n",
       "1718            0.789859          211071.0        1.086136e+07    28458932.0   \n",
       "1719            0.789859          211071.0        1.150999e+07    28487660.0   \n",
       "1720            0.907648          240152.0        1.134482e+07    28510991.0   \n",
       "1721            0.907648          240152.0        1.091275e+07    28533932.0   \n",
       "1722            0.804437          251037.0        1.129624e+07    28568175.0   \n",
       "1723            0.804437          251037.0        1.246316e+07    28597622.0   \n",
       "1724            1.012302          273051.0        1.247295e+07    28627579.0   \n",
       "1725            1.012302          273051.0        1.217682e+07    28651311.0   \n",
       "1726            0.655633          200319.0        1.312446e+07    28682122.0   \n",
       "1727            0.655633          200319.0        1.267923e+07    28701097.0   \n",
       "1728            0.768241          252113.0        1.238568e+07    28724040.0   \n",
       "1729            0.768241          252113.0        1.367660e+07    28754595.0   \n",
       "1730            0.877031          248686.0        1.136949e+07    28787814.0   \n",
       "1731            0.877031          248686.0        1.183675e+07    28815399.0   \n",
       "1732            0.693384          218339.0        1.304698e+07    28837177.0   \n",
       "1733            0.693384          218339.0        1.257106e+07    28869795.0   \n",
       "1734            0.857106          242820.0        1.353523e+07    28888424.0   \n",
       "1735            0.857106          242820.0        1.316100e+07    28923372.0   \n",
       "1736            0.838226          242303.0        1.140733e+07    28946409.0   \n",
       "1737            0.838226          242303.0        1.186584e+07    28964820.0   \n",
       "\n",
       "         sp_close      dj_close  google_trends_bitcoin  \n",
       "0     1848.359985  16576.660156               2.891802  \n",
       "1     1831.979980  16441.349609               3.417584  \n",
       "2     1831.369995  16469.990234               3.680475  \n",
       "3     1831.369995  16469.990234               3.286138  \n",
       "4     1831.369995  16469.990234               3.680475  \n",
       "5     1826.770020  16425.099609               4.469148  \n",
       "6     1837.880005  16530.939453               4.206257  \n",
       "7     1837.489990  16462.740234               4.337703  \n",
       "8     1838.130005  16444.759766               4.074812  \n",
       "9     1842.369995  16437.050781               4.074812  \n",
       "10    1842.369995  16437.050781               3.680475  \n",
       "11    1842.369995  16437.050781               3.549029  \n",
       "12    1819.199951  16257.940430               3.680475  \n",
       "13    1838.880005  16373.860352               3.623208  \n",
       "14    1848.380005  16481.939453               3.729773  \n",
       "15    1845.890015  16417.009766               4.262598  \n",
       "16    1838.699951  16458.560547               4.369163  \n",
       "17    1838.699951  16458.560547               3.196948  \n",
       "18    1838.699951  16458.560547               3.196948  \n",
       "19    1838.699951  16458.560547               3.836338  \n",
       "20    1843.800049  16414.439453               4.262598  \n",
       "21    1844.859985  16373.339844               4.475728  \n",
       "22    1828.459961  16197.349609               3.942903  \n",
       "23    1790.290039  15879.110352               3.942903  \n",
       "24    1790.290039  15879.110352               3.303513  \n",
       "25    1790.290039  15879.110352               3.303513  \n",
       "26    1781.560059  15837.879883               4.582293  \n",
       "27    1792.500000  15928.559570               5.115117  \n",
       "28    1774.199951  15738.790039               3.836338  \n",
       "29    1794.189941  15848.610352               3.516643  \n",
       "...           ...           ...                    ...  \n",
       "1708  2888.600098  25974.990234               7.699576  \n",
       "1709  2878.050049  25995.869141               8.605408  \n",
       "1710  2871.679932  25916.539062               6.642771  \n",
       "1711  2871.679932  25916.539062               5.585967  \n",
       "1712  2871.679932  25916.539062               5.434995  \n",
       "1713  2877.129883  25857.070312               6.189855  \n",
       "1714  2887.889893  25971.060547               5.887911  \n",
       "1715  2888.919922  25998.919922               6.642771  \n",
       "1716  2904.179932  26145.990234               6.340827  \n",
       "1717  2904.979980  26154.669922               5.887911  \n",
       "1718  2904.979980  26154.669922               5.133050  \n",
       "1719  2904.979980  26154.669922               4.982078  \n",
       "1720  2888.800049  26062.119141               5.585967  \n",
       "1721  2904.310059  26246.960938               5.434995  \n",
       "1722  2907.949951  26405.759766               5.736939  \n",
       "1723  2930.750000  26656.980469               5.434995  \n",
       "1724  2929.669922  26743.500000               6.189855  \n",
       "1725  2929.669922  26743.500000               5.284022  \n",
       "1726  2929.669922  26743.500000               4.982078  \n",
       "1727  2919.370117  26562.050781               5.284022  \n",
       "1728  2915.560059  26492.210938               5.736939  \n",
       "1729  2905.969971  26385.279297               5.434995  \n",
       "1730  2914.000000  26439.929688               5.284022  \n",
       "1731  2913.979980  26458.310547               5.434995  \n",
       "1732  2913.979980  26458.310547               4.529162  \n",
       "1733  2913.979980  26458.310547               4.529162  \n",
       "1734  2924.590088  26651.210938               4.831106  \n",
       "1735  2923.429932  26773.939453               4.982078  \n",
       "1736  2925.510010  26828.390625               5.133050  \n",
       "1737  2901.610107  26627.480469               4.982078  \n",
       "\n",
       "[1738 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = pd.read_csv('model_data.csv')\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_info.head()\n",
    "market_info.isnull().values.any()\n",
    "market_info.to_csv('market_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import minmax_scale, Imputer\n",
    "\n",
    "# # model_data.fillna(model_data.mean(),inplace=True)\n",
    "\n",
    "# mean_imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "\n",
    "# btc_without_date = model_data.loc[:, model_data.columns != 'Date']\n",
    "\n",
    "# # Train the imputor on the df dataset\n",
    "# mean_imputer = mean_imputer.fit(btc_without_date)\n",
    "\n",
    "# # Apply the imputer to the df dataset\n",
    "# imputed_df = mean_imputer.transform(btc_without_date)\n",
    "\n",
    "# # Get back columns\n",
    "# imputed_df = pd.DataFrame(imputed_df, columns = btc_without_date.columns)\n",
    "\n",
    "# imputed_df[['bt_Close','bt_Volume', 'sp_close', 'dj_close', 'bt_google_trends_bitcoin','bt_avg_block_size', \\\n",
    "#                                     'bt_mining_revenue', 'bt_transactions', 'bt_Accounts']] = \\\n",
    "#                 minmax_scale(imputed_df[['bt_Close', 'sp_close', 'dj_close', 'bt_Volume', \\\n",
    "#                                         'bt_mining_revenue', 'bt_google_trends_bitcoin','bt_avg_block_size', 'bt_transactions','bt_Accounts']])\n",
    "\n",
    "# # imputed_df[['bt_Close','bt_Volume', 'bt_google_trends_bitcoin']] = \\\n",
    "# #         minmax_scale(imputed_df[['bt_Close','bt_Volume', 'bt_google_trends_bitcoin']])\n",
    "\n",
    "# # imputed_df[['bt_Close','bt_Volume']] = \\\n",
    "# #         minmax_scale(imputed_df[['bt_Close','bt_Volume']])\n",
    "\n",
    "# # Re add date column\n",
    "# imputed_df['Date'] = model_data['Date']\n",
    "\n",
    "# # Order by date\n",
    "# imputed_df[\"Date\"] = imputed_df[\"Date\"].values[::-1]\n",
    "\n",
    "# model_data=imputed_df\n",
    "# # model_data.to_csv('model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>sp_close</th>\n",
       "      <th>dj_close</th>\n",
       "      <th>btc_Open</th>\n",
       "      <th>btc_High</th>\n",
       "      <th>btc_Close</th>\n",
       "      <th>btc_Volume</th>\n",
       "      <th>btc_Market Cap</th>\n",
       "      <th>btc_google_trends_bitcoin</th>\n",
       "      <th>btc_avg_block_size</th>\n",
       "      <th>btc_transactions</th>\n",
       "      <th>btc_Accounts</th>\n",
       "      <th>btc_mining_revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0.202753</td>\n",
       "      <td>0.158649</td>\n",
       "      <td>0.035372</td>\n",
       "      <td>0.035014</td>\n",
       "      <td>0.036182</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.024518</td>\n",
       "      <td>0.026115</td>\n",
       "      <td>0.031834</td>\n",
       "      <td>0.077695</td>\n",
       "      <td>0.065017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>0.190688</td>\n",
       "      <td>0.147451</td>\n",
       "      <td>0.036323</td>\n",
       "      <td>0.037261</td>\n",
       "      <td>0.037777</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.026582</td>\n",
       "      <td>0.027865</td>\n",
       "      <td>0.026115</td>\n",
       "      <td>0.031834</td>\n",
       "      <td>0.096234</td>\n",
       "      <td>0.046764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>0.190239</td>\n",
       "      <td>0.149821</td>\n",
       "      <td>0.037839</td>\n",
       "      <td>0.037952</td>\n",
       "      <td>0.038617</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.027684</td>\n",
       "      <td>0.031213</td>\n",
       "      <td>0.119025</td>\n",
       "      <td>0.088241</td>\n",
       "      <td>0.117316</td>\n",
       "      <td>0.056611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>2014-01-04</td>\n",
       "      <td>0.190239</td>\n",
       "      <td>0.149821</td>\n",
       "      <td>0.038891</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.040717</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.028449</td>\n",
       "      <td>0.025633</td>\n",
       "      <td>0.119025</td>\n",
       "      <td>0.088241</td>\n",
       "      <td>0.096473</td>\n",
       "      <td>0.051999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>2014-01-05</td>\n",
       "      <td>0.190239</td>\n",
       "      <td>0.149821</td>\n",
       "      <td>0.040709</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.044526</td>\n",
       "      <td>0.003058</td>\n",
       "      <td>0.029771</td>\n",
       "      <td>0.030097</td>\n",
       "      <td>0.088264</td>\n",
       "      <td>0.067895</td>\n",
       "      <td>0.099443</td>\n",
       "      <td>0.070736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  sp_close  dj_close  btc_Open  btc_High  btc_Close  \\\n",
       "1726  2014-01-01  0.202753  0.158649  0.035372  0.035014   0.036182   \n",
       "1725  2014-01-02  0.190688  0.147451  0.036323  0.037261   0.037777   \n",
       "1724  2014-01-03  0.190239  0.149821  0.037839  0.037952   0.038617   \n",
       "1723  2014-01-04  0.190239  0.149821  0.038891  0.039219   0.040717   \n",
       "1722  2014-01-05  0.190239  0.149821  0.040709  0.043860   0.044526   \n",
       "\n",
       "      btc_Volume  btc_Market Cap  btc_google_trends_bitcoin  \\\n",
       "1726    0.000943        0.025890                   0.024518   \n",
       "1725    0.001614        0.026582                   0.027865   \n",
       "1724    0.001586        0.027684                   0.031213   \n",
       "1723    0.001594        0.028449                   0.025633   \n",
       "1722    0.003058        0.029771                   0.030097   \n",
       "\n",
       "      btc_avg_block_size  btc_transactions  btc_Accounts  btc_mining_revenue  \n",
       "1726            0.026115          0.031834      0.077695            0.065017  \n",
       "1725            0.026115          0.031834      0.096234            0.046764  \n",
       "1724            0.119025          0.088241      0.117316            0.056611  \n",
       "1723            0.119025          0.088241      0.096473            0.051999  \n",
       "1722            0.088264          0.067895      0.099443            0.070736  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date='2018-06-01'\n",
    "# we don't need the date columns anymore\n",
    "training_set, test_set = model_data[model_data['Date']<split_date], model_data[model_data['Date']>=split_date]\n",
    "training_set = training_set.drop('Date', 1)\n",
    "test_set = test_set.drop('Date', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_len = 65\n",
    "\n",
    "LSTM_training_inputs = []\n",
    "for i in range(len(training_set)-window_len):\n",
    "    temp_set = training_set[i:(i+window_len)].copy()\n",
    "    LSTM_training_inputs.append(temp_set)   \n",
    "    \n",
    "# LSTM_training_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "LSTM_test_inputs = []\n",
    "for i in range(len(test_set)-window_len):\n",
    "    temp_set = test_set[i:(i+window_len)].copy()\n",
    "    LSTM_test_inputs.append(temp_set)\n",
    "    \n",
    "#     for col in norm_cols:\n",
    "#         temp_set.loc[:, col] = temp_set[col]/temp_set[col].iloc[0] - 1\n",
    "#     LSTM_test_inputs.append(temp_set)\n",
    "\n",
    "# LSTM_test_inputs = []\n",
    "# for i in range(len(test_set)-window_len):\n",
    "#     temp_set = test_set[i:(i+window_len)].copy()\n",
    "#     LSTM_test_inputs.append(temp_set)\n",
    "# LSTM_test_outputs = test_set['bt_Close'][window_len:].values\n",
    "# LSTM_test_outputs = test_set['bt_Close'][window_len:].values-1\n",
    "\n",
    "# print(LSTM_test_inputs[0])\n",
    "LSTM_test_outputs = test_set['btc_Close'][window_len:].values\n",
    "print(len(LSTM_test_outputs)) # predicting 45 points in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table represents an example of our LSTM model input (we'll actually have hundreds of similar tables). We've normalised some columns so that their values are equal to 0 in the first time point, so we're aiming to predict changes in price relative to this timepoint. We're now ready to build the LSTM model. This is actually quite straightforward with Keras, you simply stack componenets on top of each other (better explained [here](https://dashee87.github.io/data%20science/deep%20learning/python/another-keras-tutorial-for-neural-network-beginners/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_training_inputs = [np.array(LSTM_training_input) for LSTM_training_input in LSTM_training_inputs]\n",
    "LSTM_training_inputs = np.array(LSTM_training_inputs)\n",
    "\n",
    "LSTM_test_inputs = [np.array(LSTM_test_inputs) for LSTM_test_inputs in LSTM_test_inputs]\n",
    "LSTM_test_inputs = np.array(LSTM_test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the relevant Keras modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def build_model(inputs, output_size, neurons, activ_func=\"tanh\",\n",
    "                dropout=0.25, loss=\"mae\", optimizer=\"adam\"):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def gru_model(inputs, output_size, neurons, activ_func=\"tanh\",\n",
    "                dropout=0.25, loss=\"mae\", optimizer=\"adam\"):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(GRU(neurons, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "    model.add(Dropout(0.15))  # Dropout overfitting\n",
    "\n",
    "    # model.add(GRU(layers[2],activation='tanh', return_sequences=True))\n",
    "    # model.add(Dropout(0.2))  # Dropout overfitting\n",
    "\n",
    "    model.add(GRU(neurons, input_shape=(inputs.shape[1], inputs.shape[2]), \n",
    "                  activation='tanh', return_sequences=False))\n",
    "    model.add(Dropout(0.15))  # Dropout overfitting\n",
    "\n",
    "    model.add(Dense(output_dim=layers[3]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    # model.compile(loss=\"mse\", optimizer=sgd)\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\") # Nadam rmsprop\n",
    "    print (\"Compilation Time : \", time.time() - start)\n",
    "    return model \n",
    "\n",
    "\n",
    "\n",
    "def denser_model(inputs, output_size, neurons, activ_func='tanh', \n",
    "                 dropout=0.3, loss='mae', optimizer='adam'):\n",
    "    \"\"\"\n",
    "    inputs: input data as numpy array\n",
    "    output_size: number of predictions per input sample\n",
    "    neurons: number of neurons/ units in the LSTM layer\n",
    "    active_func: Activation function to be used in LSTM layers and Dense layer\n",
    "    dropout: dropout ration, default is 0.25\n",
    "    loss: loss function for calculating the gradient\n",
    "    optimizer: type of optimizer to backpropagate the gradient\n",
    "    This function will build 3 layered RNN model with LSTM cells with dropouts after each LSTM layer \n",
    "    and finally a dense layer to produce the output using keras' sequential model.\n",
    "    Return: Keras sequential model and model summary\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, return_sequences=True, input_shape=(inputs.shape[1], inputs.shape[2]), activation=activ_func))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(neurons, return_sequences=True, activation=activ_func))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(neurons, activation=activ_func))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(activ_func))\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['mae'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.00013822, -0.06106792,  0.10776572, ...,  0.10898142,\n",
      "        -0.09600256,  0.03442807],\n",
      "       [ 0.09821731,  0.00263345, -0.05255553, ...,  0.00569086,\n",
      "        -0.02730787, -0.04569017],\n",
      "       [ 0.0780047 , -0.0460289 ,  0.09915353, ...,  0.0528841 ,\n",
      "        -0.0464635 ,  0.08980535],\n",
      "       ...,\n",
      "       [ 0.09350048, -0.09420775,  0.01824959, ...,  0.10316053,\n",
      "        -0.00692932,  0.04098645],\n",
      "       [ 0.0526219 ,  0.03779012,  0.03271791, ..., -0.00932365,\n",
      "        -0.03446206,  0.10858494],\n",
      "       [-0.10022227, -0.03279163, -0.11224488, ..., -0.11940891,\n",
      "        -0.07235562, -0.04712335]], dtype=float32), array([[-0.13244021,  0.04335991,  0.06624217, ...,  0.04899031,\n",
      "        -0.08540058, -0.05648996],\n",
      "       [ 0.05374488,  0.03159233, -0.01711521, ...,  0.02125785,\n",
      "         0.01233767,  0.002683  ],\n",
      "       [ 0.0327259 , -0.03872018, -0.01054513, ..., -0.04347579,\n",
      "        -0.05101265, -0.01117337],\n",
      "       ...,\n",
      "       [-0.02732293, -0.00524141, -0.05093112, ..., -0.05364712,\n",
      "        -0.00802958, -0.07677154],\n",
      "       [-0.0213811 , -0.01298171,  0.00230804, ..., -0.05751638,\n",
      "        -0.06602672,  0.00464818],\n",
      "       [ 0.00315823, -0.04692919, -0.07545283, ..., -0.00807184,\n",
      "        -0.03105382,  0.07356793]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.16305791, -0.04133168,  0.15143555, ...,  0.06981599,\n",
      "         0.16727254,  0.02403536],\n",
      "       [ 0.14016768,  0.10184184, -0.06096455, ..., -0.11521683,\n",
      "         0.13345915, -0.00116158],\n",
      "       [-0.10390995,  0.0305872 ,  0.13629678, ...,  0.17671561,\n",
      "        -0.10222167,  0.02815683],\n",
      "       ...,\n",
      "       [-0.00110756, -0.16348115,  0.08669472, ...,  0.07791671,\n",
      "         0.02832073,  0.10604557],\n",
      "       [ 0.0330395 , -0.1018218 ,  0.01745491, ...,  0.10113806,\n",
      "        -0.18638518, -0.10390478],\n",
      "       [ 0.14762583, -0.16207343,  0.07863143, ..., -0.17940734,\n",
      "         0.09993434,  0.00451183]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "Epoch 1/50\n",
      " - 1s - loss: 0.0894\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.0509\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.0393\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.0340\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.0320\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.0298\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.0284\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.0256\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.0249\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.0235\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.0226\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.0246\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.0216\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.0222\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.0217\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.0217\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.0218\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.0200\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.0197\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.0196\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.0196\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.0191\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.0187\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.0191\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.0191\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.0183\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.0185\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.0195\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.0187\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.0177\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.0177\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.0171\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.0173\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.0172\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.0174\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.0166\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.0165\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.0161\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.0158\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.0160\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.0156\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.0161\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.0161\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.0161\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.0159\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.0160\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.0153\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.0155\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.0154\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.0154\n"
     ]
    }
   ],
   "source": [
    "# random seed for reproducibility\n",
    "np.random.seed(202)\n",
    "\n",
    "pred_range=60\n",
    "\n",
    "# initialise model architecture\n",
    "bt_model = build_model(LSTM_training_inputs, output_size=pred_range, neurons = 100)\n",
    "# bt_model = denser_model(LSTM_training_inputs, output_size=pred_range, neurons = 100)\n",
    "\n",
    "# model output is next 5 prices normalised to 10th previous closing price\n",
    "LSTM_training_outputs = []\n",
    "\n",
    "for i in range(window_len, len(training_set['btc_Close'])-pred_range):\n",
    "    LSTM_training_outputs.append(training_set['btc_Close'][i:i+pred_range].values)\n",
    "    \n",
    "LSTM_training_outputs = np.array(LSTM_training_outputs)\n",
    "print(bt_model.get_weights())\n",
    "# train model on data\n",
    "bt_history = bt_model.fit(LSTM_training_inputs[:-pred_range], LSTM_training_outputs, \n",
    "                            epochs=50, batch_size=120, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ann_visualizer.visualize import ann_viz;\n",
    "from keras.utils.vis_utils import plot_model\n",
    "graph = plot_model(bt_model, to_file=\"my_model.png\", show_shapes=True)\n",
    "\n",
    "#Build your model here\n",
    "# ann_viz(bt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([0,0,1,1])\n",
    "x2 = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# line, = ax.plot(x1, x2, lw=2)\n",
    "\n",
    "# ax.annotate('local max', xy=(2, 1), xytext=(3, 1.5),\n",
    "#             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "#             )\n",
    "\n",
    "ax.set_ylim(0,1.75)\n",
    "ax.set_xlim(0,1.75)\n",
    "\n",
    "# plt.scatter([0,1],[1,0], s=500,marker=\"o\", c=\"r\")\n",
    "line = plt.scatter(1.00,1.00, s=200,marker=\"o\", c=\"b\")\n",
    "line2=plt.scatter(0.00,0.00, s=200,marker=\"o\", c=\"b\")\n",
    "line3 = plt.scatter(0,1.00, s=400,marker=\"x\", c=\"r\")\n",
    "line4=plt.scatter(1.00,0.00, s=400,marker=\"x\", c=\"r\")\n",
    "line3.set_clip_on(False)\n",
    "line2.set_clip_on(False)\n",
    "line4.set_clip_on(False)\n",
    "x1, y1 = [-0.2,0.8], [0.8, -0.2]\n",
    "\n",
    "x2, y2 = [-0.2,1.5], [1.5, -0.2]\n",
    "\n",
    "line5=plt.plot(x1, y1, x2, y2, clip_on = False,color=\"green\")\n",
    "plt.title(\"XOR: Klasifikimi i pamundur nga nje vije\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 6, 6\n",
    "\n",
    "# fig.clear()\n",
    "fig, ax1 = plt.subplots(1,1)\n",
    "\n",
    "ax1.plot(bt_history.epoch, bt_history.history['loss'])\n",
    "ax1.set_title('Training Error')\n",
    "\n",
    "if bt_model.loss == 'mae':\n",
    "    ax1.set_ylabel('Mean Absolute Error (MAE)',fontsize=12)\n",
    "# just in case you decided to change the model loss calculation\n",
    "else:\n",
    "    ax1.set_ylabel('Model Loss',fontsize=12)\n",
    "ax1.set_xlabel('Nr. of Epochs',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 35, 12)\n"
     ]
    }
   ],
   "source": [
    "# params = 4 * ((len(LSTM_training_inputs[:-pred_range]) + 1) * len(LSTM_training_outputs) +  len(LSTM_training_outputs)^2)\n",
    "\n",
    "# # 4 * (4097 * 256 + 256^2) = 4457472\n",
    "# print(params)\n",
    "\n",
    "# 4 * 100 * (len(LSTM_training_inputs[:-pred_range]) + 1 + len(LSTM_training_outputs))\n",
    "\n",
    "# LSTM_training_inputs, output_size=pred_range, neurons = 100\n",
    "\n",
    "# model.add(LSTM(units=256, input_dim=4096, input_length=16))\n",
    "\n",
    "# [(256 + 4096 + 1) * 256] * 4 = 4457472\n",
    "# units=100\n",
    "# input_dim=len(LSTM_training_inputs[:-pred_range])\n",
    "# input_length=12\n",
    "              \n",
    "# p = ((units+input_dim+1)*units)*4\n",
    "              \n",
    "\n",
    "# The entities W , U and V are shared by all steps of the RNN and these are the only parameters in the model described in the figure. Hence number of parameters to be learnt while training = dim(W)+dim(V)+dim(U).\n",
    "\n",
    "# Based on data in the question this = n2+kn+nm.\n",
    "\n",
    "# where,\n",
    "\n",
    "# n - dimension of hidden layer - 100 \n",
    "# k - dimension of output layer - 30 \n",
    "# m - dimension of input layer - 1611\n",
    "\n",
    "\n",
    "d=100^2 + 30*100 + 100*1611\n",
    "d\n",
    "print(LSTM_training_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Training History: 0.022062765859034873\n",
      "Scikit: 0.012192275871242015\n",
      "Mean from Prediction: 0.0122\n",
      "dict_keys(['loss'])\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "# From History though: \n",
    "print(\"From Training History: \" + str(np.mean(bt_history.history['loss'])) )\n",
    "\n",
    "a=bt_model.predict(LSTM_training_inputs[:-pred_range])\n",
    "\n",
    "print( \"Scikit: \" + str(sklearn.metrics.mean_absolute_error(LSTM_training_outputs, bt_model.predict(LSTM_training_inputs[:-pred_range]))))\n",
    "\n",
    "print('Mean from Prediction: %.4f'%np.mean(np.abs((bt_model.predict(LSTM_training_inputs[:-pred_range]))-\\\n",
    "            (LSTM_training_outputs))))\n",
    "\n",
    "# print('MAE: %.4f'%np.mean(np.abs((bt_model.predict(LSTM_test_inputs[:-pred_range]))-\\\n",
    "#             (LSTM_training_outputs))))\n",
    "print(bt_history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~kejsistruga/12.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import sklearn.metrics\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "mae_test_error =sklearn.metrics.mean_absolute_error(LSTM_training_outputs, bt_model.predict(LSTM_training_inputs[:-pred_range]))\n",
    "\n",
    "# Create a trace\n",
    "real_price = go.Scatter(\n",
    "    x = model_data[model_data['Date']< split_date]['Date'][window_len:].astype(datetime.datetime),\n",
    "    y = training_set['btc_Close'][window_len:]+1,\n",
    "    name = \"Real Price\",\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = 'rgba(152, 0, 0, .8)'\n",
    "    )\n",
    ")\n",
    "\n",
    "predicted_price = go.Scatter(\n",
    "    x = model_data[model_data['Date']< split_date]['Date'][window_len:].astype(datetime.datetime),\n",
    "    y = ((np.transpose(bt_model.predict(LSTM_training_inputs))+1))[0], \n",
    "    name = \"Predicted Price\",\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = \"#82E0AA\"\n",
    "    )\n",
    ")\n",
    "\n",
    "layout = dict(title = 'Training Set Prediction, MAE: %.4f'%+ mae_test_error,\n",
    "              yaxis = dict(title = 'Bitcoin Price (USD)'),\n",
    "            )\n",
    "\n",
    "data = [real_price,predicted_price]\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='bitcoin-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~kejsistruga/12.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "mae_test_error =np.mean(np.abs((np.transpose(bt_model.predict(LSTM_test_inputs)))-\\\n",
    "            (test_set['btc_Close'].values[window_len:])))\n",
    "# Create a trace\n",
    "real_price = go.Scatter(\n",
    "    x = model_data[model_data['Date']>= split_date]['Date'][window_len:].astype(datetime.datetime),\n",
    "    y = test_set['btc_Close'][window_len:],\n",
    "    name = \"Real Price\",\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = 'rgba(152, 0, 0, .8)'\n",
    "    )\n",
    ")\n",
    "\n",
    "predicted_price = go.Scatter(\n",
    "    x = model_data[model_data['Date']>= split_date]['Date'][window_len:].astype(datetime.datetime),\n",
    "    y = ((np.transpose(bt_model.predict(LSTM_test_inputs)))[0]), \n",
    "    name = \"Predicted Price\",\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = \"#82E0AA\"\n",
    "    )\n",
    ")\n",
    "\n",
    "layout = dict(title = 'Test Set Prediction (Batch Size=200), MAE: %.4f'%+ mae_test_error,\n",
    "              yaxis = dict(title = 'Bitcoin Price (USD)')\n",
    "            )\n",
    "\n",
    "data = [real_price,predicted_price]\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='bitcoin-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_1 to have shape (60,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9f69658fc449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtest_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM_test_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpred_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_1 to have shape (60,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    \n",
    "    EVALUATING ON TEST SET; MAE: 0.0301\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "test_outputs=[]\n",
    "\n",
    "for i in range(window_len, len(test_set['btc_Close'])-pred_range):\n",
    "    test_outputs.append(test_set['btc_Close'][i:i+pred_range].values)\n",
    "\n",
    "a = len(test_set['btc_Close'])-pred_range\n",
    "print(a)\n",
    "test_outputs = np.array(test_outputs)\n",
    "    \n",
    "print(bt_model.evaluate(LSTM_test_inputs[:-pred_range], test_outputs, batch_size=200))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # loss: 0.030180492639249445\n",
    "# # loss with google: 0.03131183715485244\n",
    "\n",
    "# \"\"\"\n",
    "    \n",
    "#     EVALUATING ON TRAINING SET; MAE: 0.0301\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "print(bt_model.evaluate(LSTM_training_inputs[:-pred_range], LSTM_training_outputs, batch_size=200))  \n",
    "\n",
    "# # loss: 0.004505583531820338\n",
    "# # loss with google: 0.005251145405104876 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               45200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                6060      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 60)                0         \n",
      "=================================================================\n",
      "Total params: 51,260\n",
      "Trainable params: 51,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# bt_model\n",
    "bt_model.summary()\n",
    "bt_model.save('bt_model_dense.h5')\n",
    "# bt_model.layers \n",
    "# # List of input tensors:\n",
    "# bt_model.inputs\n",
    "# bt_model.outputs\n",
    "bt_model.get_weights()\n",
    "bt_model.save_weights('bt_model_weights') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "808"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(bt_model, to_file='model.png')\n",
    "bt_model = build_model(LSTM_training_inputs, output_size=pred_range, neurons = 100)\n",
    "\n",
    "def build_model(inputs, output_size, neurons, activ_func=\"tanh\",\n",
    "                dropout=0.25, loss=\"mae\", optimizer=\"adam\"):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_dim=4096, input_length=16))\n",
    "   \n",
    "    model.add(LSTM(neurons, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# inputs, output_size, neurons,\n",
    "\n",
    "# model.add(LSTM(256, input_dim=4096, input_length=16))\n",
    "\n",
    "# model.add(LSTM(256, input_dim=4096, input_length=16))\n",
    "# model.summary()\n",
    "# 4 * (4097 * 256 + 256^2) = 4457472\n",
    "# 4 * ((input_dim+1) + neurons + neurons^2)\n",
    "\n",
    "4 * ((1) * 100 + 100^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'size_of_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-84e82eee2c4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mLSTM_training_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_of_input\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msize_of_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msize_of_output\u001b[0m\u001b[0;34m^\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'size_of_input' is not defined"
     ]
    }
   ],
   "source": [
    "LSTM_training_inputs.shape[1] # 35\n",
    "LSTM_training_inputs.shape[2] # 12\n",
    "\n",
    "params = 4 * ((size_of_input + 1) * size_of_output + size_of_output^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36 + 100 + 100^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "952"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "238*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144808"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*((360 + 1) * 100 + 100^2)\n",
    "# 45200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4480"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 * (4097 * 256 + 256^2) = 4457472\n",
    "# 4097 input_dim\n",
    "128*35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7236"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*(len(LSTM_training_inputs) + 1 * 100 + 100 ^ 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420776"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4(nm+n2+n)\n",
    "# input vectors of size m\n",
    "# giving output vectors of size n\n",
    "4*(1050*100 + 100^2 + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30*35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "200ff45abb3b47f3ade09c6ce91695fb": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
